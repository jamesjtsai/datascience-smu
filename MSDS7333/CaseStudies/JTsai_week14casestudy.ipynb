{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Airline Flight Delays Case Study\n",
    "\n",
    "#### MSDS 7333 - Quantifying the World - 04/20/17\n",
    "#### Author: James Tsai, Wid Sogata     \n",
    "=================================================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Abstract\n",
    "Since October 1987, there have been over 50 million flights in the United States that failed to depart at their scheduled times. Around 200,000 of those flights were at least two hours late and some were much later. From these two simple facts we can surmise that delays are not isolated, rare events, they are routine. Since 1987 the number of flights per year has steadily increased and as this trend continues we expect to see more inconvenience, more aggravation, and more time lost.   \n",
    "\n",
    "In this case study, we are going to analyze airline data set.  We intend to have a better understanding of the cause of flight delays which could allow the airline industry to intelligently react to issues and providing more flights with fewer delays. At the same time, we are going to explore and utilize available packages in R and Python that are available to handle big data analysis.\n",
    "\n",
    "\n",
    "#### Keywords\n",
    "Big Data, Parallel Processing, Memory, Storage, Graphlab \n",
    "\n",
    "## Introduction\n",
    "In 2009, the American Statistical Association (ASA) Section on Statistical Computing and Statistical Graphics released the “Airline on-time performance” data set for their biannual data exposition. The data set was compiled and organized by Hadley Wickham from the\n",
    "official releases from the US government’s Bureau of Transportation Research and Innovative Technology Administration (RITA) Web site (http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236). The data include commercial flight information from October 1987 to April 2008 for those carriers with at least 1% of domestic U.S. flights in a given year. In total, there is information for over 120 million flights, each with 29 variables related to flight time, delay time, departure airport, arrival airport, and so on. In total, the uncompressed data set is about 12 gigabytes (GB) in size.    \n",
    "Table 1 shows the list of the variables and short description. \n",
    "\n",
    "\n",
    "|    |   Variable Name   |                                 Description                                |\n",
    "|----|:-----------------:|:--------------------------------------------------------------------------:|\n",
    "| 1  |        Year       |                                  1987-2008                                 |\n",
    "| 2  |       Month       |                                    1-12                                    |\n",
    "| 3  |     DayofMonth    |                                    1-31                                    |\n",
    "| 4  |     DayOfWeek     |                           1 (Monday) - 7 (Sunday)                          |\n",
    "| 5  |      DepTime      |                     Actual Departure Time (local, hhmm)                    |\n",
    "| 6  |     CRSDepTime    |                   Scheduled Departure Time (local, hhmm)                   |\n",
    "| 7  |      ArrTime      |                      Actual Arrival Time (local, hhmm)                     |\n",
    "| 8  |     CRSArrTime    |                    Scheduled Arrival Time (local, hhmm)                    |\n",
    "| 9  |   UniqueCarrier   |                             Unique Carrier Code                            |\n",
    "| 10 |     FlightNum     |                                Flight Number                               |\n",
    "| 11 |      TailNum      |                              Plane Tail Number                             |\n",
    "| 12 | ActualElapsedTime |                       Actual Elapsed Time, in Minutes                      |\n",
    "| 13 |   CRSElapsedTime  |                      Scheduled ElapsedTime, in Minutes                     |\n",
    "| 14 |      AirTime,     |                            Air Time, in Minutes                            |\n",
    "| 15 |      ArrDelay     |                          Arrival Delay, in Minutes                         |\n",
    "| 16 |      DepDelay     |                         Departure Delay, in Minutes                        |\n",
    "| 17 |       Origin      |                          Origin IATA Airport Code                          |\n",
    "| 18 |        Dest       |                        Destination IATA Airport Code                       |\n",
    "| 19 |      Distance     |                             Distance, in Miles                             |\n",
    "| 20 |       TaxiIn      |                          Taxi In Time, in Minutes                          |\n",
    "| 21 |      TaxiOut      |                          Taxi Out Time, in Minutes                         |\n",
    "| 22 |     Cancelled     |                          Was the Flight Cancelled?                         |\n",
    "| 23 |  CancellationCode | Reason for Cancellation  (A = Carrier, B = Weather, C = NAS, D = Security) |\n",
    "| 24 |      Diverted     |                         Diverted? (1 = Yes, 0 = No)                        |\n",
    "| 25 |    CarrierDelay   |                          Carrier Delay, in Minutes                         |\n",
    "| 26 |    WeatherDelay   |                          Weather Delay, in Minutes                         |\n",
    "| 27 |      NASDelay     |                            NAS Delay, in Minutes                           |\n",
    "| 28 |   SecurityDelay   |                         Security Delay, in Minutes                         |\n",
    "| 29 | LateAircraftDelay |                       Late Aircraft Delay, in Minutes                      |\n",
    "\n",
    "<center> Table 1. Variables and Descriptions. </center>\n",
    "<br>\n",
    "\n",
    "The data set is large that it is difficult to analyze using the standard tools and techniques we have come to rely upon. As a result, the different approach need to be utilized to understand the structure of these data. This case study presents some of these approaches and technologies along with an initial exploration of airline flight delays. \n",
    "\n",
    "In this case study, we intend to do the following:\n",
    "* Work with the airline data set (use R or Python to manage out-of-core)\n",
    "    + Collect data \n",
    "    + Prepare and clean data\n",
    "* Answer the following questions by using the split-apply-combine technique:  \n",
    "    + Which Airports are the busiest?\n",
    "    + Which Airplane is newest?\n",
    "    + Which airports are most likely to be delayed flying out of or into?      \n",
    "    + Which flights with same origin and destination are most likely to be delayed?\n",
    "    + Can you regress how delayed a flight will be before it is delayed?\n",
    "        * What are the most important features for this regression?\n",
    "        * To use meaningful evaluation criteria.\n",
    "        * Create at least one new feature variable for the regression.   \n",
    "\n",
    "We are using Python version 2.7, R version 3.3.1 and Turi Graphlab Create version 1.8.3 running on Jupyter Notebook to perform this analysis.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Method\n",
    "\n",
    "### Downloading and Preparing Dataset\n",
    "\n",
    "We manually download each of the zipped files and run the following script to download the files into a folder in the same directory as this notebook called \"data\".\n",
    "In the following blocks of code, we download the data and then decompress the files into .csv files. Each csv contains data for one year of airline flights.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# create a Data directory if not done already\n",
    "path= \"data\"\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path, 0755)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading to http://stat-computing.org/dataexpo/2009/1987.csv.bz2 to data/1987.csv.bz2\n",
      "Downloading to http://stat-computing.org/dataexpo/2009/1988.csv.bz2 to data/1988.csv.bz2\n",
      "Downloading to http://stat-computing.org/dataexpo/2009/1989.csv.bz2 to data/1989.csv.bz2\n",
      "Downloading to http://stat-computing.org/dataexpo/2009/1990.csv.bz2 to data/1990.csv.bz2\n",
      "Downloading to http://stat-computing.org/dataexpo/2009/1991.csv.bz2 to data/1991.csv.bz2\n",
      "Downloading to http://stat-computing.org/dataexpo/2009/1992.csv.bz2 to data/1992.csv.bz2\n",
      "Downloading to http://stat-computing.org/dataexpo/2009/1993.csv.bz2 to data/1993.csv.bz2\n",
      "Downloading to http://stat-computing.org/dataexpo/2009/1994.csv.bz2 to data/1994.csv.bz2\n",
      "Downloading to http://stat-computing.org/dataexpo/2009/1995.csv.bz2 to data/1995.csv.bz2\n",
      "Downloading to http://stat-computing.org/dataexpo/2009/1996.csv.bz2 to data/1996.csv.bz2\n",
      "Downloading to http://stat-computing.org/dataexpo/2009/1997.csv.bz2 to data/1997.csv.bz2\n",
      "Downloading to http://stat-computing.org/dataexpo/2009/1998.csv.bz2 to data/1998.csv.bz2\n",
      "Downloading to http://stat-computing.org/dataexpo/2009/1999.csv.bz2 to data/1999.csv.bz2\n",
      "Downloading to http://stat-computing.org/dataexpo/2009/2000.csv.bz2 to data/2000.csv.bz2\n",
      "Downloading to http://stat-computing.org/dataexpo/2009/2001.csv.bz2 to data/2001.csv.bz2\n",
      "Downloading to http://stat-computing.org/dataexpo/2009/2002.csv.bz2 to data/2002.csv.bz2\n",
      "Downloading to http://stat-computing.org/dataexpo/2009/2003.csv.bz2 to data/2003.csv.bz2\n",
      "Downloading to http://stat-computing.org/dataexpo/2009/2004.csv.bz2 to data/2004.csv.bz2\n",
      "Downloading to http://stat-computing.org/dataexpo/2009/2005.csv.bz2 to data/2005.csv.bz2\n",
      "Downloading to http://stat-computing.org/dataexpo/2009/2006.csv.bz2 to data/2006.csv.bz2\n",
      "Downloading to http://stat-computing.org/dataexpo/2009/2007.csv.bz2 to data/2007.csv.bz2\n",
      "Downloading to http://stat-computing.org/dataexpo/2009/2008.csv.bz2 to data/2008.csv.bz2\n",
      "\n",
      "\n",
      "\n",
      "total 3239736\n",
      "-rw-r--r--  1 wsogata  staff   12652442 Apr 17 08:32 1987.csv.bz2\n",
      "-rw-r--r--  1 wsogata  staff   49499025 Apr 17 08:32 1988.csv.bz2\n",
      "-rw-r--r--  1 wsogata  staff   49202298 Apr 17 08:32 1989.csv.bz2\n",
      "-rw-r--r--  1 wsogata  staff   52041322 Apr 17 08:32 1990.csv.bz2\n",
      "-rw-r--r--  1 wsogata  staff   49877448 Apr 17 08:33 1991.csv.bz2\n",
      "-rw-r--r--  1 wsogata  staff   50040946 Apr 17 08:33 1992.csv.bz2\n",
      "-rw-r--r--  1 wsogata  staff   50111774 Apr 17 08:33 1993.csv.bz2\n",
      "-rw-r--r--  1 wsogata  staff   51123887 Apr 17 08:33 1994.csv.bz2\n",
      "-rw-r--r--  1 wsogata  staff   74881752 Apr 17 08:33 1995.csv.bz2\n",
      "-rw-r--r--  1 wsogata  staff   75887707 Apr 17 08:34 1996.csv.bz2\n",
      "-rw-r--r--  1 wsogata  staff   76705687 Apr 17 08:34 1997.csv.bz2\n",
      "-rw-r--r--  1 wsogata  staff   76683506 Apr 17 08:34 1998.csv.bz2\n",
      "-rw-r--r--  1 wsogata  staff   79449438 Apr 17 08:34 1999.csv.bz2\n",
      "-rw-r--r--  1 wsogata  staff   82537924 Apr 17 08:35 2000.csv.bz2\n",
      "-rw-r--r--  1 wsogata  staff   83478700 Apr 17 08:35 2001.csv.bz2\n",
      "-rw-r--r--  1 wsogata  staff   75907218 Apr 17 08:35 2002.csv.bz2\n",
      "-rw-r--r--  1 wsogata  staff   95326801 Apr 17 08:35 2003.csv.bz2\n",
      "-rw-r--r--  1 wsogata  staff  110825331 Apr 17 08:36 2004.csv.bz2\n",
      "-rw-r--r--  1 wsogata  staff  112450321 Apr 17 08:36 2005.csv.bz2\n",
      "-rw-r--r--  1 wsogata  staff  115019195 Apr 17 08:36 2006.csv.bz2\n",
      "-rw-r--r--  1 wsogata  staff  121249243 Apr 17 08:37 2007.csv.bz2\n",
      "-rw-r--r--  1 wsogata  staff  113753229 Apr 17 08:37 2008.csv.bz2\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "\n",
    "years = range(1987,2009)  # get the years 1987 through 2008\n",
    "baseurl = \"http://stat-computing.org/dataexpo/2009/%d.csv.bz2\"\n",
    "\n",
    "files = []\n",
    "for year in years:\n",
    "    # prepare strings\n",
    "    url = baseurl%(year) # get the URL for the data file\n",
    "    save_as_filename = 'data/%d.csv.bz2'%(year)\n",
    "    files += [save_as_filename] # save name of the compressed file\n",
    "    \n",
    "    # save name of the compressed file\n",
    "    print ('Downloading to %s to %s')%(url, save_as_filename)\n",
    "    urllib.urlretrieve(url, save_as_filename)\n",
    "\n",
    "print (\"\\n\\n\")\n",
    "\n",
    "!ls -l data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We have downloaded all files needed for this analysis. We continue to prepare the data by decompressing all files in the following step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressing data/1987.csv.bz2 to data/1987.csv\n",
      "Decompressing data/1988.csv.bz2 to data/1988.csv\n",
      "Decompressing data/1989.csv.bz2 to data/1989.csv\n",
      "Decompressing data/1990.csv.bz2 to data/1990.csv\n",
      "Decompressing data/1991.csv.bz2 to data/1991.csv\n",
      "Decompressing data/1992.csv.bz2 to data/1992.csv\n",
      "Decompressing data/1993.csv.bz2 to data/1993.csv\n",
      "Decompressing data/1994.csv.bz2 to data/1994.csv\n",
      "Decompressing data/1995.csv.bz2 to data/1995.csv\n",
      "Decompressing data/1996.csv.bz2 to data/1996.csv\n",
      "Decompressing data/1997.csv.bz2 to data/1997.csv\n",
      "Decompressing data/1998.csv.bz2 to data/1998.csv\n",
      "Decompressing data/1999.csv.bz2 to data/1999.csv\n",
      "Decompressing data/2000.csv.bz2 to data/2000.csv\n",
      "Decompressing data/2001.csv.bz2 to data/2001.csv\n",
      "Decompressing data/2002.csv.bz2 to data/2002.csv\n",
      "Decompressing data/2003.csv.bz2 to data/2003.csv\n",
      "Decompressing data/2004.csv.bz2 to data/2004.csv\n",
      "Decompressing data/2005.csv.bz2 to data/2005.csv\n",
      "Decompressing data/2006.csv.bz2 to data/2006.csv\n",
      "Decompressing data/2007.csv.bz2 to data/2007.csv\n",
      "Decompressing data/2008.csv.bz2 to data/2008.csv\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "\n",
    "# decompress all the files\n",
    "for filename in files:\n",
    "    filepath = filename\n",
    "    newfilepath = filename[:-4]\n",
    "    print 'Decompressing', filepath, 'to', newfilepath\n",
    "    \n",
    "    # go through the decompressed chunks and write out to a decompressed file\n",
    "    with open(newfilepath ,'wb') as new_file, bz2.BZ2File(filepath, 'rb') as file:\n",
    "        for data in iter(lambda : file.read(100*1024), b''):\n",
    "            new_file.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We remove the compressed bz2 files from your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 23494656\r\n",
      "-rw-r--r--  1 wsogata  staff  127162942 Apr 17 08:37 1987.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  501039472 Apr 17 08:38 1988.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  486518821 Apr 17 08:39 1989.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  509194687 Apr 17 08:39 1990.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  491210093 Apr 17 08:40 1991.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  492313731 Apr 17 08:40 1992.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  490753652 Apr 17 08:41 1993.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  501558665 Apr 17 08:41 1994.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  530751568 Apr 17 08:42 1995.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  533922363 Apr 17 08:43 1996.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  540347861 Apr 17 08:43 1997.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  538432875 Apr 17 08:44 1998.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  552926022 Apr 17 08:44 1999.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  570151613 Apr 17 08:45 2000.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  600411462 Apr 17 08:46 2001.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  530507013 Apr 17 08:47 2002.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  626745242 Apr 17 08:48 2003.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  669879113 Apr 17 08:49 2004.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  671027265 Apr 17 08:51 2005.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  672068096 Apr 17 08:52 2006.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  702878193 Apr 17 08:53 2007.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  689413344 Apr 17 08:54 2008.csv\r\n"
     ]
    }
   ],
   "source": [
    "!rm data/*.bz2\n",
    "!ls -l data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The files are now available to read. We notice in some of the files there are many ascii characters such as \"@@\" and \"\\x\". We suspect that these files were corrupted when they were created or during initial compression.  \n",
    "The following block shows examples of records with ascii characted embedded in the tail number variable data in year 2001. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001,1,2,2,700,700,752,755,WN,1,N669@@,52,55,43,-3,0,DAL,HOU,239,2,7,0,NA,0,NA,NA,NA,NA,NA\n",
      "2001,1,3,3,700,700,748,755,WN,1,N658@@,48,55,35,-7,0,DAL,HOU,239,3,10,0,NA,0,NA,NA,NA,NA,NA\n",
      "2001,1,4,4,700,700,748,755,WN,1,N785@@,48,55,36,-7,0,DAL,HOU,239,6,6,0,NA,0,NA,NA,NA,NA,NA\n",
      "2001,1,5,5,700,700,754,755,WN,1,N690@@,54,55,42,-1,0,DAL,HOU,239,4,8,0,NA,0,NA,NA,NA,NA,NA\n",
      "2001,1,7,7,704,700,754,755,WN,1,N323@@,50,55,42,-1,4,DAL,HOU,239,2,6,0,NA,0,NA,NA,NA,NA,NA\n",
      "2001,1,8,1,700,700,751,755,WN,1,N685@@,51,55,38,-4,0,DAL,HOU,239,2,11,0,NA,0,NA,NA,NA,NA,NA\n",
      "2001,1,9,2,700,700,755,755,WN,1,N337@@,55,55,40,0,0,DAL,HOU,239,5,10,0,NA,0,NA,NA,NA,NA,NA\n",
      "2001,1,10,3,700,700,800,755,WN,1,N629@@,60,55,42,5,0,DAL,HOU,239,5,13,0,NA,0,NA,NA,NA,NA,NA\n",
      "2001,1,11,4,700,700,755,755,WN,1,N388@@,55,55,42,0,0,DAL,HOU,239,2,11,0,NA,0,NA,NA,NA,NA,NA\n",
      "2001,1,12,5,700,700,750,755,WN,1,N621@@,50,55,39,-5,0,DAL,HOU,239,3,8,0,NA,0,NA,NA,NA,NA,NA\n",
      "\n",
      "2001,1,5,5,1327,1255,1548,1516,US,379,-N922�,81,81,66,32,32,MKE,PIT,431,6,9,0,NA,0,NA,NA,NA,NA,NA\n",
      "2001,1,1,1,1913,1915,1958,2003,US,397,-N970�,45,48,33,-5,-2,CRW,PIT,164,7,5,0,NA,0,NA,NA,NA,NA,NA\n",
      "2001,1,2,2,1913,1915,2002,2003,US,397,-N912�,49,48,38,-1,-2,CRW,PIT,164,7,4,0,NA,0,NA,NA,NA,NA,NA\n",
      "2001,1,3,3,1912,1915,2004,2003,US,397,-N927�,52,48,41,1,-3,CRW,PIT,164,5,6,0,NA,0,NA,NA,NA,NA,NA\n",
      "2001,1,4,4,1914,1915,1955,2003,US,397,-N919�,41,48,30,-8,-1,CRW,PIT,164,7,4,0,NA,0,NA,NA,NA,NA,NA\n",
      "2001,1,5,5,1910,1915,2007,2003,US,397,-N913�,57,48,43,4,-5,CRW,PIT,164,7,7,0,NA,0,NA,NA,NA,NA,NA\n",
      "2001,1,6,6,1915,1915,1951,2003,US,397,-N934�,36,48,24,-12,0,CRW,PIT,164,7,5,0,NA,0,NA,NA,NA,NA,NA\n",
      "2001,1,7,7,1914,1915,2003,2003,US,397,-N923�,49,48,32,0,-1,CRW,PIT,164,10,7,0,NA,0,NA,NA,NA,NA,NA\n",
      "2001,1,8,1,1925,1915,2009,2003,US,397,-N991�,44,48,33,6,10,CRW,PIT,164,6,5,0,NA,0,NA,NA,NA,NA,NA\n",
      "2001,1,9,2,1911,1915,2004,2003,US,397,-N919�,53,48,43,1,-4,CRW,PIT,164,5,5,0,NA,0,NA,NA,NA,NA,NA\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "grep -m 10 \"@@\" data/2001.csv \n",
    "echo \"\"\n",
    "grep -m 10 \"\\-N\" data/2001.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ideally, we should consult with subject matter expert in order to attain the best way to proceed. In this case, we decided to simply eliminate these ascii characters from all files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removal completed ...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Removing unwanted and ASCII characters.\n",
    "\n",
    "for year in {1987..2008}\n",
    "do \n",
    "# echo \"Processing file $year.csv ...\"\n",
    "  perl -pi -e 's/[^[:ascii:]]//g' data/$year.csv\n",
    "  perl -pi -e 's/'\\-N'/N/g' data/$year.csv\n",
    "  perl -pi -e 's/'@@'//g' data/$year.csv\n",
    "done\n",
    "echo \"Removal completed ...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We the run the same command to ensure there are no more ascii characters in any files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "grep -m 10 \"@@\" data/2001.csv \n",
    "echo \"\"\n",
    "grep -m 10 \"\\-N\" data/2001.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The command didn't retun any output as expected, we can assume that the files are now clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loading Data Into Memory\n",
    "Now that the data has downloaded, decompressed and cleaned, we can load a single file into memory to ensure that everything decompressed correctly. For each file, we could load it into memory and then save the length of the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Loading individual files in python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rmagic extension is already loaded. To reload it, use:\n",
      "  %reload_ext rmagic\n",
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    }
   ],
   "source": [
    "# Enable R extension\n",
    "%load_ext rmagic\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to replace all the strings in the data frame, we first need to know exactly all the unique strings contained in each column. The first block takes a pass on all the data and finds all the unique string entries in the entire dataset. This means loading all the data files in one pass and concatenating the unique entries into a data structure. I have chosen to use a dictionary as the data structure with the name of each key in the dictionary being the column name of the variable we want to convert to an integer. The value of the key is a set of strings. Once we have this dataset, we can then replace the unique values properly (we will need to load the data files again). We can then replace the values with an integer. After replacing them, we can resave the data frame as a csv. \n",
    "The runtime of these blocks aggregates to ~60 minutes (or more) depending on your system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data/1987.csv ...finding unique values\n",
      "...finished, 3.77 seconds\n",
      "loading data/1988.csv ...finding unique values\n",
      "...finished, 14.65 seconds\n",
      "loading data/1989.csv ...finding unique values\n",
      "...finished, 13.49 seconds\n",
      "loading data/1990.csv ...finding unique values\n",
      "...finished, 14.16 seconds\n",
      "loading data/1991.csv ...finding unique values\n",
      "...finished, 13.95 seconds\n",
      "loading data/1992.csv ...finding unique values\n",
      "...finished, 13.75 seconds\n",
      "loading data/1993.csv ...finding unique values\n",
      "...finished, 13.57 seconds\n",
      "loading data/1994.csv ...finding unique values\n",
      "...finished, 13.79 seconds\n",
      "loading data/1995.csv ...finding unique values\n",
      "...finished, 13.94 seconds\n",
      "loading data/1996.csv ...finding unique values\n",
      "...finished, 14.06 seconds\n",
      "loading data/1997.csv ...finding unique values\n",
      "...finished, 14.57 seconds\n",
      "loading data/1998.csv ...finding unique values\n",
      "...finished, 14.27 seconds\n",
      "loading data/1999.csv ...finding unique values\n",
      "...finished, 14.52 seconds\n",
      "loading data/2000.csv ...finding unique values\n",
      "...finished, 15.64 seconds\n",
      "loading data/2001.csv ...finding unique values\n",
      "...finished, 15.36 seconds\n",
      "loading data/2002.csv ...finding unique values\n",
      "...finished, 13.69 seconds\n",
      "loading data/2003.csv ...finding unique values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wsogata/anaconda/envs/2_7/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...finished, 17.44 seconds\n",
      "loading data/2004.csv ...finding unique values\n",
      "...finished, 18.81 seconds\n",
      "loading data/2005.csv ...finding unique values\n",
      "...finished, 18.91 seconds\n",
      "loading data/2006.csv ...finding unique values\n",
      "...finished, 18.85 seconds\n",
      "loading data/2007.csv ...finding unique values\n",
      "...finished, 20.33 seconds\n",
      "loading data/2008.csv ...finding unique values\n",
      "...finished, 19.29 seconds\n"
     ]
    }
   ],
   "source": [
    "unique_values = {} # create an empty dictionary of the column name an the unique\n",
    "for year in range(1987,2009):\n",
    "    t = time.time()\n",
    "    # get file name of the csv\n",
    "    csvfile = \"data/%d.csv\"%(year)\n",
    "    print 'loading',csvfile,\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # read the file\n",
    "    df = pd.read_csv(csvfile,usecols=['Origin', 'Dest', 'UniqueCarrier','TailNum','CancellationCode'])\n",
    "    #df = df.select_dtypes(exclude=['float64','int64']) # grab only the non-numeric print '...finding unique values',\n",
    "\n",
    "    print '...finding unique values'\n",
    "    sys.stdout.flush()\n",
    "                                      \n",
    "    for col in df.columns:\n",
    "        # check to see if we have seen this column before\n",
    "        s = set(df[col].values.astype(np.str))\n",
    "        if col not in unique_values:\n",
    "            # if not, then create a key with the unique values for that column in\n",
    "            unique_values[col] = s\n",
    "        else:\n",
    "            # otherwise make sure that the remaining columns are unique\n",
    "            unique_values[col] |= s\n",
    "                                      \n",
    "    print '...finished, %.2f seconds'%(time.time()-t)\n",
    "    sys.stdout.flush()\n",
    "    del df\n",
    "                                      \n",
    "# Save out the dictionary for later use\n",
    "pickle.dump(unique_values, open(\"data/unique_mapping.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Origin', 'TailNum', 'UniqueCarrier', 'Dest', 'CancellationCode']\n",
      "One example: set(['A', 'C', 'B', 'D', 'nan'])\n"
     ]
    }
   ],
   "source": [
    "# Check the dictionary\n",
    "print unique_values.keys()\n",
    "print 'One example:',unique_values['CancellationCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 23482264\r\n",
      "-rw-r--r--  1 wsogata  staff  127162942 Apr 17 08:54 1987.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  501039472 Apr 17 08:55 1988.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  486518821 Apr 17 08:55 1989.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  509194687 Apr 17 08:55 1990.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  491210093 Apr 17 08:55 1991.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  492313731 Apr 17 08:55 1992.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  490753652 Apr 17 08:56 1993.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  501558665 Apr 17 08:56 1994.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  530751568 Apr 17 08:56 1995.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  533922363 Apr 17 08:56 1996.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  540347861 Apr 17 08:57 1997.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  538432875 Apr 17 08:57 1998.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  552926022 Apr 17 08:57 1999.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  570151613 Apr 17 08:57 2000.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  594042864 Apr 17 08:58 2001.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  529597879 Apr 17 08:58 2002.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  626745242 Apr 17 08:58 2003.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  669879113 Apr 17 08:59 2004.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  671027265 Apr 17 08:59 2005.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  672068096 Apr 17 08:59 2006.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  702878193 Apr 17 09:00 2007.csv\r\n",
      "-rw-r--r--  1 wsogata  staff  689413344 Apr 17 09:00 2008.csv\r\n",
      "-rw-r--r--  1 wsogata  staff     933764 Apr 20 09:54 unique_mapping.p\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can see that a unique entries file had been created. Now we are ready to combine CSV files into one file. In the following function, we use fast set comparison using numpy to try and speed up the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fast_numpy_replace(np_vector, replace_set):\n",
    "    replace_set = np.array(list(replace_set))\n",
    "    n = np.ndarray(np_vector.shape).astype(np.float64)\n",
    "\n",
    "    vector_as_set, idx_back = np.unique(np_vector, return_inverse = True)\n",
    "    \n",
    "    for idx, val in enumerate(vector_as_set):\n",
    "        category_num = np.nonzero(replace_set == val)[0][0]\n",
    "        n[idx_back == idx] = category_num\n",
    "\n",
    "    return n.astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The following block code makes one large file with the numeric data. It also solves a problem with pandas closing the file that takes an inordinate amount of time. Using binary here would be an option to speedup the process, but currently backing file for bigmatrix is unstable, so we use CSV format instead.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running... data/1987.csv loaded,... replacing values Orig Tail Uniq Dest Canc ... writing ...\n",
      ", 52.11 sec/\n",
      "running... data/1988.csv loaded,... replacing values Orig Tail Uniq Dest Canc ... writing ...\n",
      ", 218.62 sec/\n",
      "running... data/1989.csv loaded,... replacing values Orig Tail Uniq Dest Canc ... writing ...\n",
      ", 225.45 sec/\n",
      "running... data/1990.csv loaded,... replacing values Orig Tail Uniq Dest Canc ... writing ...\n",
      ", 243.08 sec/\n",
      "running... data/1991.csv loaded,... replacing values Orig Tail Uniq Dest Canc ... writing ...\n",
      ", 224.63 sec/\n",
      "running... data/1992.csv loaded,... replacing values Orig Tail Uniq Dest Canc ... writing ...\n",
      ", 242.91 sec/\n",
      "running... data/1993.csv loaded,... replacing values Orig Tail Uniq Dest Canc ... writing ...\n",
      ", 220.18 sec/\n",
      "running... data/1994.csv loaded,... replacing values Orig Tail Uniq Dest Canc ... writing ...\n",
      ", 226.11 sec/\n",
      "running... data/1995.csv loaded,... replacing values Orig Tail Uniq Dest Canc ... writing ...\n",
      ", 290.63 sec/\n",
      "running... data/1996.csv loaded,... replacing values Orig Tail Uniq Dest Canc ... writing ...\n",
      ", 291.27 sec/\n",
      "running... data/1997.csv loaded,... replacing values Orig Tail Uniq Dest Canc ... writing ...\n",
      ", 283.41 sec/\n",
      "running... data/1998.csv loaded,... replacing values Orig Tail Uniq Dest Canc ... writing ...\n",
      ", 268.57 sec/\n",
      "running... data/1999.csv loaded,... replacing values Orig Tail Uniq Dest Canc ... writing ...\n",
      ", 278.05 sec/\n",
      "running... data/2000.csv loaded,... replacing values Orig Tail Uniq Dest Canc ... writing ...\n",
      ", 284.04 sec/\n",
      "running... data/2001.csv loaded,... replacing values Orig Tail Uniq Dest Canc ... writing ...\n",
      ", 298.37 sec/\n",
      "running... data/2002.csv loaded,... replacing values Orig Tail Uniq Dest Canc ... writing ...\n",
      ", 287.25 sec/\n",
      "running... data/2003.csv loaded,... replacing values Orig Tail Uniq Dest Canc ... writing ...\n",
      ", 360.00 sec/\n",
      "running... data/2004.csv loaded,... replacing values Orig Tail Uniq Dest Canc ... writing ...\n",
      ", 420.16 sec/\n",
      "running... data/2005.csv loaded,... replacing values Orig Tail Uniq Dest Canc ... writing ...\n",
      ", 423.65 sec/\n",
      "running... data/2006.csv loaded,... replacing values Orig Tail Uniq Dest Canc ... writing ...\n",
      ", 430.00 sec/\n",
      "running... data/2007.csv loaded,... replacing values Orig Tail Uniq Dest Canc ... writing ...\n",
      ", 459.77 sec/\n",
      "running... data/2008.csv loaded,... replacing values Orig Tail Uniq Dest Canc ... writing ...\n",
      ", 389.21 sec/\n",
      "closing file ...\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "fileHandle = open(\"data/AirlineDataAll.csv\", \"w\")\n",
    "years = range(1987,2009)\n",
    "for year in years:\n",
    "    t = time.time()\n",
    "    \n",
    "    # get file name of the csv\n",
    "    csvfile = 'data/%d.csv'%(year)\n",
    "    print 'running...', csvfile,\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # read the file\n",
    "    df = pd.read_csv(csvfile)\n",
    "    \n",
    "    print 'loaded,... replacing values',\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # now replace the matching columnar data with the proper number category\n",
    "    for key in unique_values.keys():\n",
    "        if key in df:\n",
    "            print key[0:4],\n",
    "            sys.stdout.flush()\n",
    "            tmp = df[key].values.astype(np.str)\n",
    "            df[key] = fast_numpy_replace(tmp, unique_values[key])\n",
    "            \n",
    "    print '...',\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    for col in df:\n",
    "        df[col] = np.round(df[col].astype(np.float64))\n",
    "        \n",
    "    print 'writing ...'\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # these lines make one large file with the numeric data\n",
    "\n",
    "    if year == years[0]:\n",
    "        df.to_csv(fileHandle,\n",
    "                  index=False,\n",
    "                  index_label=False,\n",
    "                  na_rep=\"NA\",\n",
    "                  float_format='%.0f')\n",
    "    else:\n",
    "        df.to_csv(fileHandle,\n",
    "                  mode='a',\n",
    "                  header=False,\n",
    "                  index=False,\n",
    "                  index_label=False,\n",
    "                  na_rep=\"NA\",\n",
    "                  float_format='%.0f')\n",
    "           \n",
    "    print ', %.2f sec/'%(time.time()-t)\n",
    "    del df\n",
    "\n",
    "print 'closing file ...'\n",
    "sys.stdout.flush()\n",
    "    \n",
    "fileHandle.close()\n",
    "print 'Done...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we take a look to see what has actually changed in the file. We load the head of 1987 and the big CSV file to see how they compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New File Format:\n",
      "Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay\n",
      "1987,10,14,3,741,730,912,849,7,1451,5407,91,79,NA,23,11,172,202,447,NA,NA,0,4,0,NA,NA,NA,NA,NA\n",
      "1987,10,15,4,729,730,903,849,7,1451,5407,94,79,NA,14,-1,172,202,447,NA,NA,0,4,0,NA,NA,NA,NA,NA\n",
      "1987,10,17,6,741,730,918,849,7,1451,5407,97,79,NA,29,11,172,202,447,NA,NA,0,4,0,NA,NA,NA,NA,NA\n",
      "1987,10,18,7,729,730,847,849,7,1451,5407,78,79,NA,-2,-1,172,202,447,NA,NA,0,4,0,NA,NA,NA,NA,NA\n",
      "1987,10,19,1,749,730,922,849,7,1451,5407,93,79,NA,33,19,172,202,447,NA,NA,0,4,0,NA,NA,NA,NA,NA\n",
      "1987,10,21,3,728,730,848,849,7,1451,5407,80,79,NA,-1,-2,172,202,447,NA,NA,0,4,0,NA,NA,NA,NA,NA\n",
      "1987,10,22,4,728,730,852,849,7,1451,5407,84,79,NA,3,-2,172,202,447,NA,NA,0,4,0,NA,NA,NA,NA,NA\n",
      "1987,10,23,5,731,730,902,849,7,1451,5407,91,79,NA,13,1,172,202,447,NA,NA,0,4,0,NA,NA,NA,NA,NA\n",
      "1987,10,24,6,744,730,908,849,7,1451,5407,84,79,NA,19,14,172,202,447,NA,NA,0,4,0,NA,NA,NA,NA,NA\n",
      "\n",
      "Old file format:\n",
      "Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay\n",
      "1987,10,14,3,741,730,912,849,PS,1451,NA,91,79,NA,23,11,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1987,10,15,4,729,730,903,849,PS,1451,NA,94,79,NA,14,-1,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1987,10,17,6,741,730,918,849,PS,1451,NA,97,79,NA,29,11,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1987,10,18,7,729,730,847,849,PS,1451,NA,78,79,NA,-2,-1,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1987,10,19,1,749,730,922,849,PS,1451,NA,93,79,NA,33,19,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1987,10,21,3,728,730,848,849,PS,1451,NA,80,79,NA,-1,-2,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1987,10,22,4,728,730,852,849,PS,1451,NA,84,79,NA,3,-2,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1987,10,23,5,731,730,902,849,PS,1451,NA,91,79,NA,13,1,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1987,10,24,6,744,730,908,849,PS,1451,NA,84,79,NA,19,14,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n"
     ]
    }
   ],
   "source": [
    "print 'New File Format:'\n",
    "!head data/AirlineDataAll.csv\n",
    "print \"\"\n",
    "print \"Old file format:\"\n",
    "!head data/1987.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We now look at the tail of our big dataset and the tail of the 2008 file. They seem to compare nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New file format:\n",
      "2008,12,13,6,1007,847,1149,1010,0,1631,3004,162,143,122,99,80,251,20,689,8,32,0,4,0,1,0,19,0,79\n",
      "2008,12,13,6,638,640,808,753,0,1632,7115,90,73,50,15,-2,130,255,270,14,26,0,4,0,0,0,15,0,0\n",
      "2008,12,13,6,756,800,1032,1026,0,1633,1851,96,86,56,6,-4,304,255,425,23,17,0,4,0,NA,NA,NA,NA,NA\n",
      "2008,12,13,6,612,615,923,907,0,1635,5221,131,112,103,16,-3,53,272,546,5,23,0,4,0,0,0,16,0,0\n",
      "2008,12,13,6,749,750,901,859,0,1636,5891,72,69,41,2,-1,180,255,215,20,11,0,4,0,NA,NA,NA,NA,NA\n",
      "2008,12,13,6,1002,959,1204,1150,0,1636,5891,122,111,71,14,3,251,21,533,6,45,0,4,0,NA,NA,NA,NA,NA\n",
      "2008,12,13,6,834,835,1021,1023,0,1637,6002,167,168,139,-2,-1,251,185,874,5,23,0,4,0,NA,NA,NA,NA,NA\n",
      "2008,12,13,6,655,700,856,856,0,1638,11175,121,116,85,0,-5,235,255,545,24,12,0,4,0,NA,NA,NA,NA,NA\n",
      "2008,12,13,6,1251,1240,1446,1437,0,1639,5891,115,117,89,9,11,21,255,533,13,13,0,4,0,NA,NA,NA,NA,NA\n",
      "2008,12,13,6,1110,1103,1413,1418,0,1641,6002,123,135,104,-5,7,181,255,874,8,11,0,4,0,NA,NA,NA,NA,NA\n",
      "\n",
      "Old file format:\n",
      "1987,12,5,6,1530,1530,1820,1823,CO,638,NA,110,113,NA,-3,0,ORD,EWR,719,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1987,12,6,7,1530,1530,1820,1823,CO,638,NA,110,113,NA,-3,0,ORD,EWR,719,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1987,12,7,1,1530,1530,1823,1823,CO,638,NA,113,113,NA,0,0,ORD,EWR,719,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1987,12,8,2,1530,1530,1820,1823,CO,638,NA,110,113,NA,-3,0,ORD,EWR,719,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1987,12,10,4,1530,1530,1823,1823,CO,638,NA,113,113,NA,0,0,ORD,EWR,719,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1987,12,11,5,1530,1530,1825,1823,CO,638,NA,115,113,NA,2,0,ORD,EWR,719,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1987,12,13,7,1530,1530,1815,1823,CO,638,NA,105,113,NA,-8,0,ORD,EWR,719,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1987,12,14,1,1530,1530,1807,1823,CO,638,NA,97,113,NA,-16,0,ORD,EWR,719,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1987,12,1,2,1525,1525,1643,1638,CO,639,NA,78,73,NA,5,0,BOS,EWR,200,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n",
      "1987,12,2,3,1540,1525,1706,1638,CO,639,NA,86,73,NA,28,15,BOS,EWR,200,NA,NA,0,NA,0,NA,NA,NA,NA,NA\n"
     ]
    }
   ],
   "source": [
    "print \"New file format:\"\n",
    "!tail data/AirlineDataAll.csv\n",
    "print \"\"\n",
    "print \"Old file format:\"\n",
    "!tail data/1987.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the above code, we created a large file with all the data from every year inside of it. This is now saved as `AirlineDataAll`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 wsogata  staff  11780086039 Apr 20 11:41 Data/AirlineDataAll.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -all Data/*All.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we can see that the 'AirlineDataAll' file above is about 12GB. Loading it into main memory like normal is not an option, so we will create some aggregations in both R and python. We start with the bigmemory package in R.\n",
    "<br>\n",
    "<br>\n",
    "We need to create a binary file that memory maps the data we are interested in or known as 'backing file'. To create that binary backing file we will read the data using the 'big.matrix' package.  \n",
    "The operation below will take about 25 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wsogata/anaconda/envs/2_7/lib/python2.7/site-packages/rpy2/rinterface/__init__.py:186: RRuntimeWarning: Loading required package: bigmemory.sri\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1] 123534969        30\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "library(bigmemory)\n",
    "#library(bigtabulate)\n",
    "\n",
    "x = read.big.matrix(\"data/AirlineDataAll.csv\", \n",
    "                    header = TRUE,\n",
    "                    backingpath = \"data\", \n",
    "                    backingfile = \"airline.bin\",\n",
    "                    descriptorfile = \"airline.desc\", \n",
    "                    type = \"integer\",\n",
    "                    extraCols = \"age\")\n",
    "print (dim(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now lets look at the files created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 wsogata  staff  14824196281 Apr 20 12:16 data/airline.bin\r\n",
      "-rw-r--r--  1 wsogata  staff          900 Apr 20 11:50 data/airline.desc\r\n"
     ]
    }
   ],
   "source": [
    "ls -l data/airline*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Analyzing Busiest Airport\n",
    "Using big memory is similar to using R data.frames, but there is decreased functionality. Basically, we need to use the optimized functions such as bigsplit in place of split whenever possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rmagic extension is already loaded. To reload it, use:\n",
      "  %reload_ext rmagic\n",
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import cPickle as pickle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%load_ext rmagic\n",
    "%load_ext rpy2.ipython\n",
    "\n",
    "unique_values = pickle.load( open( \"Data/unique_mapping.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's now use bigmemory and bigtabulate to start the process and start grouping our massive matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(bigmemory)\n",
    "library(bigtabulate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, we attach the binary backing file through its descriptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "x <- attach.big.matrix(\"data/airline.desc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We split up the x data based on the unique airport it flew from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "origin_indices = bigsplit(x, 'Origin', splitcol = NA_real_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We then do the count from the Origin variable, which shows us the busiest airport by sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%R -o counts\n",
    "counts = sapply(origin_indices, function(i) length(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAewAAAD8CAYAAAC1i5dPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2AAA2ACBDQCADRDYAADYwP8Bk0PnW8CMmm8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103f7d250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "idxs = np.argsort(counts)[-10:]\n",
    "counts_large = counts[idxs]\n",
    "\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "plt.barh(range(len(counts_large)), counts_large)\n",
    "\n",
    "xticks_labels = np.array(list(unique_values['Origin']))[idxs]\n",
    "\n",
    "plt.yticks(range(len(xticks_labels)), xticks_labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<center> figure 1. Plot of Busiest Airports.</center>  \n",
    "Plot in Figure 1 shows DLG, HPN and JAN as the top 3 busiest aiports. All 3 of them show over 5 millions flight departures and the top 2 with over 6 millions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Analyzing Age of Plane\n",
    "\n",
    "We now go ahead and calculate the age of the different planes when they took their flights. We will estimate the first flight of the plane in the dataset as its 'birthmonth'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "acindices <- bigsplit(x, 'TailNum', splitcol = NA_real_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "On the next code block, we define the birthmonth function. We use the very first recorded flight for a specific plane as an estimate of its age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "birthmonth <- function(y) {\n",
    "    # assume that the input is one matric of values from one plane\n",
    "    \n",
    "    # get minimum year for this plane\n",
    "    minYear <- min(y[,'Year'], na.rm=TRUE)\n",
    "    # get a subset of the dataset for only this minimum year\n",
    "    these <- which(y[,'Year']==minYear)\n",
    "    # get the minimum month from the years\n",
    "    minMonth <- min(y[these,'Month'], na.rm=TRUE) \n",
    "    # return the number of months since 00 AD \n",
    "    return(12*minYear + minMonth - 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the following, we use sapply that will send the different indices into the given function. We send in the indices of the plane, with the year and month. This will be done for every plane based upon the acindices grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "acStart = sapply(acindices, function(i) birthmonth(x[i, c('Year', 'Month'), drop = FALSE]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can significantly improve the process by running it in the parallel computation. In the next block we split the operations using a foreach parallel loop using doMC package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%R \n",
    "library(doMC)\n",
    "registerDoMC(cores=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%R -o acStart\n",
    "acStart <- foreach(i=acindices, .combine = c)%dopar% {\n",
    "    return(birthmonth(x[i, c('Year', 'Month'), drop=FALSE]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We noticed only mild speedup because the computation is fairly quick. We then find the the youngest place by using argmax function of numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The youngest plane is N290WN and flew starting in the year 2009\n"
     ]
    }
   ],
   "source": [
    "idx = np.argmax(acStart)\n",
    "\n",
    "print 'The youngest plane is', list(unique_values['TailNum'])[idx],\n",
    "print 'and flew starting in the year %.0f'%(acStart[idx]/12.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The result shows flight number N290WN as the newest plane and it started operation in the year 2009. On the next block of code, we fill in the values for 'age' in the big.matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Assignment will down cast from double to integer\n",
       "Hint: To remove this warning type:  options(bigmemory.typecast.warning=FALSE)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "x[,\"age\"] <- x[,\"Year\"]*12+x[,\"Month\"]-acStart[x[,\"TailNum\"]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "This operation was possible because we used the `extraCols` argument when we created the backing file for the big.matrix. If we has not told the API that we planned to add this extra column, then the performance hit for creating a new column is HUGE. That's because we would need to completetly recreate the binary backing file on the hard drive. By telling the API we wnated it upfront, it went ahead and stored NaNs in the column when it created the binary file. That means we just needed to update the file incstead of write a complete new one.\n",
    "\n",
    "They are easy to perform analysis on them using the technique of splitting, applying, and combining (essentially this is map-reduce in an embarassingly parallel context). Once the preprocessing is done it is also extremely easy (and fast) to load the memory mapped file onto into your R session. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Linear Regression of Airline Data\n",
    "For our Regression analysis, we create a regression model based upon the age of the plane. We want to know if some of the factors such as age are siginificant factors in determining if it will have a delay associated with it. We can achieve this in R by using the 'biganalytics' package. This package uses batch analysis (like we talked about and used in Data Mining) in order to use mini-batch gradient descent upon the linear regression model.\n",
    "<br>\n",
    "<br>\n",
    "In the following step, we will try to predict arrival delay (ArrDelay) using age, distance, weather delat and security airport delay.\n",
    "<br>\n",
    "<center> *ArrDelay = B0(intercept) + B1(Age) + B2(Distance) + B3(WeatherDelay) + B4(SecurityDelay)* </center>\n",
    "<br>\n",
    "<br>\n",
    "In the following, biganalytics provides a wrapper to the biglm package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "blm <- biglm.big.matrix( ArrDelay ~ age + Distance + WeatherDelay + SecurityDelay , data=x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Large data regression model: biglm(formula = formula, data = data, ...)\n",
       "Sample size =  33540215 \n",
       "                Coef   (95%    CI)     SE      p\n",
       "(Intercept)   6.8532 6.8271 6.8793 0.0130 0.0000\n",
       "age           0.0324 0.0321 0.0326 0.0001 0.0000\n",
       "Distance      0.0000 0.0000 0.0000 0.0000 0.3424\n",
       "WeatherDelay  1.1446 1.1433 1.1459 0.0007 0.0000\n",
       "SecurityDelay 1.0699 1.0594 1.0803 0.0052 0.0000\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "summary(blm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The summary of the regression analysis shows the coefficients for the intercept and variables in included. It shows that there is a positive association of age and delays, but the association is rather weak. For the distance, the result shows there is no correlation as the p-value exceeds value of 0.05 with 95% confidence level. As for the last 2 features, WeatherDelay and SecurityDelay, we see a positive associatian as expected.   \n",
    "Given the results, we can now construct the model based on the features given.\n",
    "<br>\n",
    "<center> *ArrDelay = 6.85 + 0.0324(Age) + 1.145(WeatherDelay) + 1.070(SecurityDelay)* </center>\n",
    "<br>\n",
    "\n",
    "For future endeavor, it is possible to expand our analysis to other numeric variables that we are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Analyzing data using Turi Graphlab\n",
    "\n",
    "Now we will analyze the data using a different package called Graphlab Create (from the company Turi). The graphlab-create API uses something called a scalable data frame (or SFrame) that handles all of the out-of-core memory management. It is by far one of the most optimized tools for handling table data out-of-core. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import graphlab as gl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, we load up aour data into Graphlab SFrame data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to wsogata@smu.edu and will expire on August 06, 2017.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: /tmp/graphlab_server_1492716270.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/AirlineDataAll.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/AirlineDataAll.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 1.51946 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 1.51946 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[int,int,int,int,int,int,int,int,int,int,int,int,int,str,int,int,int,int,int,str,str,int,int,int,str,str,str,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 541148 lines. Lines per second: 165438</pre>"
      ],
      "text/plain": [
       "Read 541148 lines. Lines per second: 165438"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 2719289 lines. Lines per second: 297239</pre>"
      ],
      "text/plain": [
       "Read 2719289 lines. Lines per second: 297239"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 4903426 lines. Lines per second: 325953</pre>"
      ],
      "text/plain": [
       "Read 4903426 lines. Lines per second: 325953"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 7071440 lines. Lines per second: 339781</pre>"
      ],
      "text/plain": [
       "Read 7071440 lines. Lines per second: 339781"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 9253168 lines. Lines per second: 346982</pre>"
      ],
      "text/plain": [
       "Read 9253168 lines. Lines per second: 346982"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 11421982 lines. Lines per second: 349999</pre>"
      ],
      "text/plain": [
       "Read 11421982 lines. Lines per second: 349999"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 13600297 lines. Lines per second: 354347</pre>"
      ],
      "text/plain": [
       "Read 13600297 lines. Lines per second: 354347"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 15775671 lines. Lines per second: 357978</pre>"
      ],
      "text/plain": [
       "Read 15775671 lines. Lines per second: 357978"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 17942465 lines. Lines per second: 361320</pre>"
      ],
      "text/plain": [
       "Read 17942465 lines. Lines per second: 361320"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 20120148 lines. Lines per second: 363723</pre>"
      ],
      "text/plain": [
       "Read 20120148 lines. Lines per second: 363723"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 22283807 lines. Lines per second: 365348</pre>"
      ],
      "text/plain": [
       "Read 22283807 lines. Lines per second: 365348"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 24461038 lines. Lines per second: 367292</pre>"
      ],
      "text/plain": [
       "Read 24461038 lines. Lines per second: 367292"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 26628985 lines. Lines per second: 367144</pre>"
      ],
      "text/plain": [
       "Read 26628985 lines. Lines per second: 367144"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 28800588 lines. Lines per second: 367743</pre>"
      ],
      "text/plain": [
       "Read 28800588 lines. Lines per second: 367743"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 30973522 lines. Lines per second: 369080</pre>"
      ],
      "text/plain": [
       "Read 30973522 lines. Lines per second: 369080"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 33136496 lines. Lines per second: 370394</pre>"
      ],
      "text/plain": [
       "Read 33136496 lines. Lines per second: 370394"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 35310913 lines. Lines per second: 371780</pre>"
      ],
      "text/plain": [
       "Read 35310913 lines. Lines per second: 371780"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 37471275 lines. Lines per second: 371451</pre>"
      ],
      "text/plain": [
       "Read 37471275 lines. Lines per second: 371451"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 39662352 lines. Lines per second: 372109</pre>"
      ],
      "text/plain": [
       "Read 39662352 lines. Lines per second: 372109"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 41847975 lines. Lines per second: 372375</pre>"
      ],
      "text/plain": [
       "Read 41847975 lines. Lines per second: 372375"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 44034568 lines. Lines per second: 373448</pre>"
      ],
      "text/plain": [
       "Read 44034568 lines. Lines per second: 373448"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 46221577 lines. Lines per second: 372533</pre>"
      ],
      "text/plain": [
       "Read 46221577 lines. Lines per second: 372533"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 48397980 lines. Lines per second: 373354</pre>"
      ],
      "text/plain": [
       "Read 48397980 lines. Lines per second: 373354"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 50582923 lines. Lines per second: 373285</pre>"
      ],
      "text/plain": [
       "Read 50582923 lines. Lines per second: 373285"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 52758423 lines. Lines per second: 372813</pre>"
      ],
      "text/plain": [
       "Read 52758423 lines. Lines per second: 372813"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 54940715 lines. Lines per second: 373300</pre>"
      ],
      "text/plain": [
       "Read 54940715 lines. Lines per second: 373300"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 57124357 lines. Lines per second: 373592</pre>"
      ],
      "text/plain": [
       "Read 57124357 lines. Lines per second: 373592"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 59291994 lines. Lines per second: 373582</pre>"
      ],
      "text/plain": [
       "Read 59291994 lines. Lines per second: 373582"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 61477252 lines. Lines per second: 373804</pre>"
      ],
      "text/plain": [
       "Read 61477252 lines. Lines per second: 373804"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 63651456 lines. Lines per second: 374386</pre>"
      ],
      "text/plain": [
       "Read 63651456 lines. Lines per second: 374386"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 65823033 lines. Lines per second: 374992</pre>"
      ],
      "text/plain": [
       "Read 65823033 lines. Lines per second: 374992"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 67999667 lines. Lines per second: 375076</pre>"
      ],
      "text/plain": [
       "Read 67999667 lines. Lines per second: 375076"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 70161671 lines. Lines per second: 375551</pre>"
      ],
      "text/plain": [
       "Read 70161671 lines. Lines per second: 375551"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 72337163 lines. Lines per second: 375548</pre>"
      ],
      "text/plain": [
       "Read 72337163 lines. Lines per second: 375548"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 74514019 lines. Lines per second: 375865</pre>"
      ],
      "text/plain": [
       "Read 74514019 lines. Lines per second: 375865"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 76674412 lines. Lines per second: 376151</pre>"
      ],
      "text/plain": [
       "Read 76674412 lines. Lines per second: 376151"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 78844498 lines. Lines per second: 376525</pre>"
      ],
      "text/plain": [
       "Read 78844498 lines. Lines per second: 376525"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 80999611 lines. Lines per second: 376716</pre>"
      ],
      "text/plain": [
       "Read 80999611 lines. Lines per second: 376716"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 83167682 lines. Lines per second: 376192</pre>"
      ],
      "text/plain": [
       "Read 83167682 lines. Lines per second: 376192"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 85413151 lines. Lines per second: 375474</pre>"
      ],
      "text/plain": [
       "Read 85413151 lines. Lines per second: 375474"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 87677217 lines. Lines per second: 375055</pre>"
      ],
      "text/plain": [
       "Read 87677217 lines. Lines per second: 375055"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 89957626 lines. Lines per second: 374472</pre>"
      ],
      "text/plain": [
       "Read 89957626 lines. Lines per second: 374472"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 92239255 lines. Lines per second: 374470</pre>"
      ],
      "text/plain": [
       "Read 92239255 lines. Lines per second: 374470"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 94505251 lines. Lines per second: 374244</pre>"
      ],
      "text/plain": [
       "Read 94505251 lines. Lines per second: 374244"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 96783381 lines. Lines per second: 373833</pre>"
      ],
      "text/plain": [
       "Read 96783381 lines. Lines per second: 373833"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 99061091 lines. Lines per second: 373151</pre>"
      ],
      "text/plain": [
       "Read 99061091 lines. Lines per second: 373151"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 100765747 lines. Lines per second: 372119</pre>"
      ],
      "text/plain": [
       "Read 100765747 lines. Lines per second: 372119"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 101891940 lines. Lines per second: 368653</pre>"
      ],
      "text/plain": [
       "Read 101891940 lines. Lines per second: 368653"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 103599464 lines. Lines per second: 367413</pre>"
      ],
      "text/plain": [
       "Read 103599464 lines. Lines per second: 367413"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 105308906 lines. Lines per second: 366960</pre>"
      ],
      "text/plain": [
       "Read 105308906 lines. Lines per second: 366960"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 107016340 lines. Lines per second: 366443</pre>"
      ],
      "text/plain": [
       "Read 107016340 lines. Lines per second: 366443"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 109272595 lines. Lines per second: 366505</pre>"
      ],
      "text/plain": [
       "Read 109272595 lines. Lines per second: 366505"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 111547161 lines. Lines per second: 366189</pre>"
      ],
      "text/plain": [
       "Read 111547161 lines. Lines per second: 366189"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 113254612 lines. Lines per second: 365606</pre>"
      ],
      "text/plain": [
       "Read 113254612 lines. Lines per second: 365606"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 115520198 lines. Lines per second: 365221</pre>"
      ],
      "text/plain": [
       "Read 115520198 lines. Lines per second: 365221"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 117185504 lines. Lines per second: 363812</pre>"
      ],
      "text/plain": [
       "Read 117185504 lines. Lines per second: 363812"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 118822178 lines. Lines per second: 362901</pre>"
      ],
      "text/plain": [
       "Read 118822178 lines. Lines per second: 362901"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 121003415 lines. Lines per second: 362977</pre>"
      ],
      "text/plain": [
       "Read 121003415 lines. Lines per second: 362977"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 123164985 lines. Lines per second: 362848</pre>"
      ],
      "text/plain": [
       "Read 123164985 lines. Lines per second: 362848"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/AirlineDataAll.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/AirlineDataAll.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 123534969 lines in 340.266 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 123534969 lines in 340.266 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sf = gl.SFrame('data/AirlineDataAll.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can run shape command to check the dimension of the frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123534969, 29)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The results show exactly the same dimension as the previous steps in big matrix format.  \n",
    "<br>\n",
    "The next block code to perform all the concatenation we did earlier. We are also supplying graphlab with the data type for each column to ensure that the data is consistent. However, if we do not, Graphlab would try and guess the data type based on the first 100 lines of the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 0 lines, reading next file 1987.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 540959 lines. Lines per second: 163251</pre>"
      ],
      "text/plain": [
       "Read 540959 lines. Lines per second: 163251"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1987.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1987.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 1311826 lines in 4.55464 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 1311826 lines in 4.55464 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 1311826 lines, reading next file 1988.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 546089 lines. Lines per second: 166835</pre>"
      ],
      "text/plain": [
       "Read 546089 lines. Lines per second: 166835"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 2730418 lines. Lines per second: 281140</pre>"
      ],
      "text/plain": [
       "Read 2730418 lines. Lines per second: 281140"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 4362442 lines. Lines per second: 297101</pre>"
      ],
      "text/plain": [
       "Read 4362442 lines. Lines per second: 297101"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1988.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1988.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 5202096 lines in 16.1195 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 5202096 lines in 16.1195 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 6513922 lines, reading next file 1989.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 545019 lines. Lines per second: 162688</pre>"
      ],
      "text/plain": [
       "Read 545019 lines. Lines per second: 162688"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 2724002 lines. Lines per second: 285166</pre>"
      ],
      "text/plain": [
       "Read 2724002 lines. Lines per second: 285166"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 4890812 lines. Lines per second: 322268</pre>"
      ],
      "text/plain": [
       "Read 4890812 lines. Lines per second: 322268"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1989.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1989.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 5041200 lines in 15.393 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 5041200 lines in 15.393 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 11555122 lines, reading next file 1990.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 1089088 lines. Lines per second: 207373</pre>"
      ],
      "text/plain": [
       "Read 1089088 lines. Lines per second: 207373"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 3265792 lines. Lines per second: 275620</pre>"
      ],
      "text/plain": [
       "Read 3265792 lines. Lines per second: 275620"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1990.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1990.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 5270893 lines in 17.1603 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 5270893 lines in 17.1603 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 16826015 lines, reading next file 1991.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 543797 lines. Lines per second: 151522</pre>"
      ],
      "text/plain": [
       "Read 543797 lines. Lines per second: 151522"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 2715831 lines. Lines per second: 267150</pre>"
      ],
      "text/plain": [
       "Read 2715831 lines. Lines per second: 267150"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 4878544 lines. Lines per second: 302240</pre>"
      ],
      "text/plain": [
       "Read 4878544 lines. Lines per second: 302240"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1991.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1991.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 5076925 lines in 16.4205 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 5076925 lines in 16.4205 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 21902940 lines, reading next file 1992.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 1087617 lines. Lines per second: 224291</pre>"
      ],
      "text/plain": [
       "Read 1087617 lines. Lines per second: 224291"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 3263042 lines. Lines per second: 294592</pre>"
      ],
      "text/plain": [
       "Read 3263042 lines. Lines per second: 294592"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1992.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1992.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 5092157 lines in 15.2903 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 5092157 lines in 15.2903 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 26995097 lines, reading next file 1993.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 543425 lines. Lines per second: 168659</pre>"
      ],
      "text/plain": [
       "Read 543425 lines. Lines per second: 168659"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 2716517 lines. Lines per second: 289412</pre>"
      ],
      "text/plain": [
       "Read 2716517 lines. Lines per second: 289412"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 4877507 lines. Lines per second: 326548</pre>"
      ],
      "text/plain": [
       "Read 4877507 lines. Lines per second: 326548"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1993.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1993.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 5070501 lines in 15.2166 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 5070501 lines in 15.2166 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 32065598 lines, reading next file 1994.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 1086678 lines. Lines per second: 219295</pre>"
      ],
      "text/plain": [
       "Read 1086678 lines. Lines per second: 219295"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 3258677 lines. Lines per second: 288917</pre>"
      ],
      "text/plain": [
       "Read 3258677 lines. Lines per second: 288917"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1994.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1994.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 5180048 lines in 15.6968 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 5180048 lines in 15.6968 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 37245646 lines, reading next file 1995.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 527246 lines. Lines per second: 168203</pre>"
      ],
      "text/plain": [
       "Read 527246 lines. Lines per second: 168203"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 2635922 lines. Lines per second: 299147</pre>"
      ],
      "text/plain": [
       "Read 2635922 lines. Lines per second: 299147"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 4739268 lines. Lines per second: 327272</pre>"
      ],
      "text/plain": [
       "Read 4739268 lines. Lines per second: 327272"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1995.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1995.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 5327435 lines in 15.5818 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 5327435 lines in 15.5818 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 42573081 lines, reading next file 1996.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 530202 lines. Lines per second: 172066</pre>"
      ],
      "text/plain": [
       "Read 530202 lines. Lines per second: 172066"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 2636325 lines. Lines per second: 303892</pre>"
      ],
      "text/plain": [
       "Read 2636325 lines. Lines per second: 303892"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 4733913 lines. Lines per second: 332452</pre>"
      ],
      "text/plain": [
       "Read 4733913 lines. Lines per second: 332452"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1996.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1996.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 5351983 lines in 15.5325 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 5351983 lines in 15.5325 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 47925064 lines, reading next file 1997.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 529207 lines. Lines per second: 167887</pre>"
      ],
      "text/plain": [
       "Read 529207 lines. Lines per second: 167887"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 2632920 lines. Lines per second: 292629</pre>"
      ],
      "text/plain": [
       "Read 2632920 lines. Lines per second: 292629"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 4729684 lines. Lines per second: 321624</pre>"
      ],
      "text/plain": [
       "Read 4729684 lines. Lines per second: 321624"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1997.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1997.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 5411843 lines in 16.2158 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 5411843 lines in 16.2158 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 53336907 lines, reading next file 1998.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 528414 lines. Lines per second: 168798</pre>"
      ],
      "text/plain": [
       "Read 528414 lines. Lines per second: 168798"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 2629594 lines. Lines per second: 295775</pre>"
      ],
      "text/plain": [
       "Read 2629594 lines. Lines per second: 295775"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 4724179 lines. Lines per second: 323246</pre>"
      ],
      "text/plain": [
       "Read 4724179 lines. Lines per second: 323246"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1998.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1998.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 5384721 lines in 16.0382 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 5384721 lines in 16.0382 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 58721628 lines, reading next file 1999.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 524911 lines. Lines per second: 165973</pre>"
      ],
      "text/plain": [
       "Read 524911 lines. Lines per second: 165973"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 2627946 lines. Lines per second: 292627</pre>"
      ],
      "text/plain": [
       "Read 2627946 lines. Lines per second: 292627"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 4723791 lines. Lines per second: 318448</pre>"
      ],
      "text/plain": [
       "Read 4723791 lines. Lines per second: 318448"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1999.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/1999.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 5527884 lines in 16.5504 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 5527884 lines in 16.5504 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 64249512 lines, reading next file 2000.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 524495 lines. Lines per second: 169445</pre>"
      ],
      "text/plain": [
       "Read 524495 lines. Lines per second: 169445"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 2619994 lines. Lines per second: 296840</pre>"
      ],
      "text/plain": [
       "Read 2619994 lines. Lines per second: 296840"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 4710482 lines. Lines per second: 324854</pre>"
      ],
      "text/plain": [
       "Read 4710482 lines. Lines per second: 324854"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/2000.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/2000.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 5683047 lines in 16.8993 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 5683047 lines in 16.8993 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 69932559 lines, reading next file 2001.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 527597 lines. Lines per second: 167980</pre>"
      ],
      "text/plain": [
       "Read 527597 lines. Lines per second: 167980"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 2638227 lines. Lines per second: 295567</pre>"
      ],
      "text/plain": [
       "Read 2638227 lines. Lines per second: 295567"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 4753139 lines. Lines per second: 317993</pre>"
      ],
      "text/plain": [
       "Read 4753139 lines. Lines per second: 317993"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/2001.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/2001.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 5967780 lines in 17.922 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 5967780 lines in 17.922 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 75900339 lines, reading next file 2002.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 528611 lines. Lines per second: 166381</pre>"
      ],
      "text/plain": [
       "Read 528611 lines. Lines per second: 166381"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 2619976 lines. Lines per second: 295824</pre>"
      ],
      "text/plain": [
       "Read 2619976 lines. Lines per second: 295824"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 4703277 lines. Lines per second: 321972</pre>"
      ],
      "text/plain": [
       "Read 4703277 lines. Lines per second: 321972"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/2002.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/2002.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 5271359 lines in 15.6873 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 5271359 lines in 15.6873 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 81171698 lines, reading next file 2003.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 522593 lines. Lines per second: 163359</pre>"
      ],
      "text/plain": [
       "Read 522593 lines. Lines per second: 163359"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 2614044 lines. Lines per second: 287888</pre>"
      ],
      "text/plain": [
       "Read 2614044 lines. Lines per second: 287888"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 4290310 lines. Lines per second: 297752</pre>"
      ],
      "text/plain": [
       "Read 4290310 lines. Lines per second: 297752"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 5959346 lines. Lines per second: 301062</pre>"
      ],
      "text/plain": [
       "Read 5959346 lines. Lines per second: 301062"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/2003.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/2003.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 6488540 lines in 21.3303 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 6488540 lines in 21.3303 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 87660238 lines, reading next file 2004.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 560245 lines. Lines per second: 147614</pre>"
      ],
      "text/plain": [
       "Read 560245 lines. Lines per second: 147614"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 2238808 lines. Lines per second: 222970</pre>"
      ],
      "text/plain": [
       "Read 2238808 lines. Lines per second: 222970"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 3917196 lines. Lines per second: 248518</pre>"
      ],
      "text/plain": [
       "Read 3917196 lines. Lines per second: 248518"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 5594351 lines. Lines per second: 257959</pre>"
      ],
      "text/plain": [
       "Read 5594351 lines. Lines per second: 257959"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/2004.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/2004.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 7129270 lines in 26.7819 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 7129270 lines in 26.7819 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 94789508 lines, reading next file 2005.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 560141 lines. Lines per second: 164618</pre>"
      ],
      "text/plain": [
       "Read 560141 lines. Lines per second: 164618"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 2239266 lines. Lines per second: 248270</pre>"
      ],
      "text/plain": [
       "Read 2239266 lines. Lines per second: 248270"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 3917795 lines. Lines per second: 266028</pre>"
      ],
      "text/plain": [
       "Read 3917795 lines. Lines per second: 266028"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 5594330 lines. Lines per second: 274314</pre>"
      ],
      "text/plain": [
       "Read 5594330 lines. Lines per second: 274314"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/2005.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/2005.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 7140596 lines in 25.484 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 7140596 lines in 25.484 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 101930104 lines, reading next file 2006.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 558959 lines. Lines per second: 158283</pre>"
      ],
      "text/plain": [
       "Read 558959 lines. Lines per second: 158283"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 2235783 lines. Lines per second: 235472</pre>"
      ],
      "text/plain": [
       "Read 2235783 lines. Lines per second: 235472"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 3913588 lines. Lines per second: 257213</pre>"
      ],
      "text/plain": [
       "Read 3913588 lines. Lines per second: 257213"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 5586441 lines. Lines per second: 268992</pre>"
      ],
      "text/plain": [
       "Read 5586441 lines. Lines per second: 268992"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/2006.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/2006.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 7141922 lines in 25.9814 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 7141922 lines in 25.9814 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 109072026 lines, reading next file 2007.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 559204 lines. Lines per second: 164319</pre>"
      ],
      "text/plain": [
       "Read 559204 lines. Lines per second: 164319"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 2233455 lines. Lines per second: 243901</pre>"
      ],
      "text/plain": [
       "Read 2233455 lines. Lines per second: 243901"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 3905970 lines. Lines per second: 263208</pre>"
      ],
      "text/plain": [
       "Read 3905970 lines. Lines per second: 263208"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 5576065 lines. Lines per second: 270590</pre>"
      ],
      "text/plain": [
       "Read 5576065 lines. Lines per second: 270590"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 7230606 lines. Lines per second: 276545</pre>"
      ],
      "text/plain": [
       "Read 7230606 lines. Lines per second: 276545"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/2007.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/2007.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 7453215 lines in 26.692 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 7453215 lines in 26.692 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 116525241 lines, reading next file 2008.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 535634 lines. Lines per second: 167251</pre>"
      ],
      "text/plain": [
       "Read 535634 lines. Lines per second: 167251"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 2141844 lines. Lines per second: 248799</pre>"
      ],
      "text/plain": [
       "Read 2141844 lines. Lines per second: 248799"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 3746137 lines. Lines per second: 263822</pre>"
      ],
      "text/plain": [
       "Read 3746137 lines. Lines per second: 263822"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 5346855 lines. Lines per second: 270778</pre>"
      ],
      "text/plain": [
       "Read 5346855 lines. Lines per second: 270778"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 6931041 lines. Lines per second: 276333</pre>"
      ],
      "text/plain": [
       "Read 6931041 lines. Lines per second: 276333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/2008.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/wsogata/Learn/Git/Quant/Unit14/data/2008.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 7009728 lines in 25.3613 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 7009728 lines in 25.3613 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 434.29 seconds to concacenate the memory mapped file\n",
      "Saving... \n",
      "It took 62.32 seconds Shape of Sframe is  (123534969, 29)\n"
     ]
    }
   ],
   "source": [
    "del sf \n",
    "column_hints = [long,long,long,long,long,long,long,long,str,long,str,long,long,str,long,long,str,str,long,str,str,long,str,long,str,str,str,str,str]\n",
    "t = time.time()\n",
    "sf = gl.SFrame()\n",
    "\n",
    "for year in range(1987,2009):\n",
    "    print 'read %d lines, reading next file %d.csv'%(sf.shape[0], year)\n",
    "    sys.stdout.flush()\n",
    "    sftmp = gl.SFrame.read_csv('data/%d.csv'%(year), column_type_hints=column_hints)\n",
    "    sf = sf.append(sftmp)\n",
    "    \n",
    "print 'It took %.2f seconds to concacenate the memory mapped file'%(time.time()-t)\n",
    "t = time.time()\n",
    "print 'Saving... '\n",
    "sf.save('data/sframe_directory')\n",
    "print 'It took %.2f seconds'%(time.time()-t), 'Shape of Sframe is ', sf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We were able to concatenate and load the file in ~430 seconds and saved a compressed binary version in about 1 minute.  \n",
    "\n",
    "To demonstrate this Graphlab package, for the next step we would like repeat linear regression analysis and to find the answer on which flights with same origin and destination are most likely to be delayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Linear Regression Analysis using Graphlab\n",
    "We are only going to regress feature Age and Distance agains arrival delay in this demonstration for the sake of saving time.   \n",
    "<br>\n",
    "<center> *ArrDelay = B0(intercept) + B1(Age) + B2(Distance)* </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sf = gl.load_sframe('data/sframe_directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "On the following step, we only use years where the tail number was recorded and only use variables needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sf_tmp = sf[['TailNum','Year','Month','ArrDelay', 'Distance',]][sf['Year']>1994]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We then make a function for getting the age of the plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 19.51 seconds to run\n"
     ]
    }
   ],
   "source": [
    "# First we save the plane's age in years\n",
    "sf_tmp['FlightAge'] = 12*sf_tmp['Year']+sf_tmp['Month']-1\n",
    "\n",
    "# # and take the minimum of that in order to get its first flight\n",
    "t = time.time()\n",
    "sf_min_ages = sf_tmp[['TailNum','FlightAge']].groupby('TailNum',{'FirstFlight':gl.aggregate.MIN('FlightAge')})\n",
    "print 'Took %.2f seconds to run'%(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, we transform the FirstFlight Column into the original dataframe size. We save the flight age and the minimum in a new SFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 50s, sys: 20.1 s, total: 10min 11s\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "%time sf_fewcols = sf_tmp[['TailNum','FlightAge']].join(sf_min_ages,on='TailNum',how='left') # long operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We subtract the new calculated quantity and add to the original SFrame. And finally we run regression on age and distance against delay time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sf_tmp['Age'] = sf_fewcols['FlightAge']-sf_fewcols['FirstFlight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Creating a validation set from 5 percent of training data. This may take a while.\n",
      "          You can set ``validation_set=None`` to disable validation tracking.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Linear regression:</pre>"
      ],
      "text/plain": [
       "Linear regression:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of examples          : 80001380</pre>"
      ],
      "text/plain": [
       "Number of examples          : 80001380"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of features          : 2</pre>"
      ],
      "text/plain": [
       "Number of features          : 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of unpacked features : 2</pre>"
      ],
      "text/plain": [
       "Number of unpacked features : 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Number of coefficients    : 3</pre>"
      ],
      "text/plain": [
       "Number of coefficients    : 3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Starting Newton Method</pre>"
      ],
      "text/plain": [
       "Starting Newton Method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>--------------------------------------------------------</pre>"
      ],
      "text/plain": [
       "--------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+----------------------+---------------+-----------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+----------------------+---------------+-----------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| Iteration | Passes   | Elapsed Time | Training-max_error | Validation-max_error | Training-rmse | Validation-rmse |</pre>"
      ],
      "text/plain": [
       "| Iteration | Passes   | Elapsed Time | Training-max_error | Validation-max_error | Training-rmse | Validation-rmse |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+----------------------+---------------+-----------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+----------------------+---------------+-----------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>| 1         | 2        | 8.832490     | 2589.482905        | 1466.010289          | 33.362170     | 33.357393       |</pre>"
      ],
      "text/plain": [
       "| 1         | 2        | 8.832490     | 2589.482905        | 1466.010289          | 33.362170     | 33.357393       |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>+-----------+----------+--------------+--------------------+----------------------+---------------+-----------------+</pre>"
      ],
      "text/plain": [
       "+-----------+----------+--------------+--------------------+----------------------+---------------+-----------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>SUCCESS: Optimal solution found.</pre>"
      ],
      "text/plain": [
       "SUCCESS: Optimal solution found."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 31s, sys: 7.08 s, total: 4min 38s\n",
      "Wall time: 1min 18s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre></pre>"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%time lin_model = gl.linear_regression.create(sf_tmp['ArrDelay','Age','Distance'].dropna(), target='ArrDelay', features=['Age', 'Distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">name</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">index</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">value</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">stderr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">(intercept)</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">None</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7.12567872013</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.00723062182091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Age</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">None</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0114750105505</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">9.00304595085e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Distance</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">None</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.000335106452285</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6.63944413468e-06</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[3 rows x 4 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tname\tstr\n",
       "\tindex\tstr\n",
       "\tvalue\tfloat\n",
       "\tstderr\tfloat\n",
       "\n",
       "Rows: 3\n",
       "\n",
       "Data:\n",
       "+-------------+-------+--------------------+-------------------+\n",
       "|     name    | index |       value        |       stderr      |\n",
       "+-------------+-------+--------------------+-------------------+\n",
       "| (intercept) |  None |   7.12567872013    |  0.00723062182091 |\n",
       "|     Age     |  None |  0.0114750105505   | 9.00304595085e-05 |\n",
       "|   Distance  |  None | -0.000335106452285 | 6.63944413468e-06 |\n",
       "+-------------+-------+--------------------+-------------------+\n",
       "[3 rows x 4 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_model['coefficients']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<center> Table2. Regression Coefficents of Arrival Delay Features.</center>\n",
    "\n",
    "The result is consistent tht gives us a positive association of Age and arrival delay time. The value is of the same magnitude as what we found using R.   \n",
    "On Distance coefficient, it is rather different, it maybe because we need to use gradient descent (not batch methods, which are deterministic). The number of iterations and starting point for gradient methods will slightly (or not so slightly sometimes) affect the final coefficient values. Additionally We also highly suspect the pvalue is insignificant, but we cannot tell definitely by using this package. Thus, we decide to leave this feature and the regression model is  \n",
    "<br>\n",
    "<center> *ArrDelay = 6.750 + 0.005(Age)* </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Which flights with same origin and destination are most likely to be delayed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import graphlab.aggregate as agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First we group the Origin and Dest and calculate the mean delay for each leg and number of FlightNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 46s, sys: 3.76 s, total: 1min 50s\n",
      "Wall time: 35.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sf_delay1 = sf.groupby(key_columns=['Origin','Dest'], operations={ 'mean_delay': agg.MEAN('DepDelay'),\n",
    "                                                                   'count_FlightNum': agg.COUNT('FlightNum')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next we only want to keep the legs that have more than one flight and return the leg with highest mean delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 63.1 ms, sys: 20.2 ms, total: 83.3 ms\n",
      "Wall time: 60.4 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Dest</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Origin</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">mean_delay</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">count_FlightNum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">FWA</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">ABE</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">211.5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">BIS</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">RAP</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">208.333333333</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">ABQ</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">FMN</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">203.666666667</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">MFR</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">RDD</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">179.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">FAT</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">OAK</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">176.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">PIH</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">FAT</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">176.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">SGU</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">LAS</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">175.333333333</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">ROA</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">GSO</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">174.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">COD</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">BIL</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">173.333333333</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">MSO</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">HLN</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">165.666666667</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[7636 rows x 4 columns]<br/>Note: Only the head of the SFrame is printed.<br/>You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tDest\tstr\n",
       "\tOrigin\tstr\n",
       "\tmean_delay\tfloat\n",
       "\tcount_FlightNum\tint\n",
       "\n",
       "Rows: 7636\n",
       "\n",
       "Data:\n",
       "+------+--------+---------------+-----------------+\n",
       "| Dest | Origin |   mean_delay  | count_FlightNum |\n",
       "+------+--------+---------------+-----------------+\n",
       "| FWA  |  ABE   |     211.5     |        2        |\n",
       "| BIS  |  RAP   | 208.333333333 |        3        |\n",
       "| ABQ  |  FMN   | 203.666666667 |        3        |\n",
       "| MFR  |  RDD   |     179.0     |        3        |\n",
       "| FAT  |  OAK   |     176.0     |        4        |\n",
       "| PIH  |  FAT   |     176.0     |        2        |\n",
       "| SGU  |  LAS   | 175.333333333 |        3        |\n",
       "| ROA  |  GSO   |     174.0     |        2        |\n",
       "| COD  |  BIL   | 173.333333333 |        3        |\n",
       "| MSO  |  HLN   | 165.666666667 |        4        |\n",
       "+------+--------+---------------+-----------------+\n",
       "[7636 rows x 4 columns]\n",
       "Note: Only the head of the SFrame is printed.\n",
       "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sf_delay1[sf_delay1['count_FlightNum']>1].sort('mean_delay', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We see that ABE->FWA has the highest mean delay of 211.5 We can now group the flights with just ABE->FWA on the original SFrame and calculate the mean delay for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 40s, sys: 1.53 s, total: 1min 41s\n",
      "Wall time: 27.2 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">FlightNum</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">mean_delay</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6818</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5932</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">366.0</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[2 rows x 2 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tFlightNum\tint\n",
       "\tmean_delay\tfloat\n",
       "\n",
       "Rows: 2\n",
       "\n",
       "Data:\n",
       "+-----------+------------+\n",
       "| FlightNum | mean_delay |\n",
       "+-----------+------------+\n",
       "|    6818   |    57.0    |\n",
       "|    5932   |   366.0    |\n",
       "+-----------+------------+\n",
       "[2 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sf[(sf['Origin']=='ABE') & (sf['Dest']=='FWA')].groupby(key_columns=['FlightNum'],\n",
    "                                                        operations={ 'mean_delay': agg.MEAN('DepDelay') })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "The result shows flights 6818 and 5932 are most likely delayed and they have the same Origin of 'ABE' and Destination of 'FWA'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Big data sets can challenge and frustrate data analytic users even on high performance hardware.\n",
    "Certain low-level programming languages such as C++ can be helpful, but is cumbersome for interactive\n",
    "data analysis and lacks the flexibility and power of the rich statistical programming environment in Python and R.\n",
    "The R package bigmemory aims to bridge this gap, implementing massive matrices in memory (managed in R but programmed in C++) and supporting their basic manipulation and exploration. Graphlab is another popular platform managed on Python and also programmed in C++ for using scalable machine learning and big data analysis. They are ideal for most problems involving the analysis of manageable subsets of the data, or when an analysis conducted in C++ becomes too complicated.\n",
    "\n",
    "Given many benefits of these big data packages, they do come with limitations. Certain analysis may not be appropriate for the built-in algorithm on these packages. Some of the versatilities that can be found in packages such as Pandas and Numpy also may not be available and hardware resources such memory might become a constraint.\n",
    "\n",
    "In conclusion, now that we have seen R's bigmemory and Graphlab's SFrame,  we have a new set of tools to tackle massive data that we are likely to encounter. Performing preprocessing with SFrames is almost always recommended. They are easier to work with. However, the features in split-apply-combine is still falls short of the versatilities of R's bigmemory package. Some operations are optimized, and if we can get away with only using those operations, then try just using SFrames. On the other hand, bigmemory is more flexible and versatile for more deep-dived data mining tasks. The newer version of bigmemory implements the data structures in shared memory. The shared memory version of a BigMatrix object will allow separate R processes on the same computer to share access to a single copy of the massive data set in memory. This opens the door for more powerful and efficient parallel analyses and data mining of massive data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### References\n",
    "\n",
    "Nolan, D. & Lang, T. L. (2015) Data Science in R. A Case Studies Approach to Computational Reasoning and Problem Solving. Boca Raton, FL: CRC Press.  \n",
    "Wickham, W. & Grolemund, G. (2015) R for Data Science. Sebastopol, CA: OReilly Media, Inc.   \n",
    "Case Studies in Data Science with R, Retrieved March 24, 2017, from http://rdatasciencecases.org/   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
